# code/agents/filejob_agent.py

import os
import tempfile
import zipfile
import shutil
import json
import logging
import asyncio
import re
import fnmatch
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
from utils.markdown_utils import escape_markdown_v2
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton
from aiogram import types, F
from aiogram.types import FSInputFile
from aiogram.fsm.context import FSMContext

logger = logging.getLogger(__name__)

class FilejobAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –ò–ò –∫ –∫–∞–∂–¥–æ–º—É —Ñ–∞–π–ª—É"""
    
    def __init__(self):
        self.name = "filejob"
        self.config = self._load_default_config()
        self.active_jobs = {}  # –•—Ä–∞–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–º–µ–Ω—ã
        
    async def handle_document(self, message: types.Message, state: FSMContext, agent_handler):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è filejob –∞–≥–µ–Ω—Ç–∞
        –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –ø—Ä–æ—Å—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        """
        await message.reply(
            "‚ÑπÔ∏è –ê–≥–µ–Ω—Ç *filejob* –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É —Ñ–∞–π–ª–æ–≤\\.\n\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:\n"
            "`@filejob —Å–µ–ª–µ–∫—Ç–æ—Ä_—Ñ–∞–π–ª–æ–≤`\n"
            "–¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ –∞–∫—Ç–∏–≤–Ω–æ–π –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã\\.\n\n"
            "_–ï—Å–ª–∏ —É –≤–∞—Å –∞–∫—Ç–∏–≤–µ–Ω –¥—Ä—É–≥–æ–π –∞–≥–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /agent\\_stop_",
            parse_mode="MarkdownV2"
        )

    def _load_default_config(self) -> Dict[str, Any]:
        config_path = os.path.join(os.path.dirname(__file__), "configs", "filejob_default.json")
        
        default_config = {}
        
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)
                    for key, value in loaded_config.items():
                        default_config[key] = value
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞ Filejob –∞–≥–µ–Ω—Ç–∞: {e}")
                
        return default_config
    
    def _parse_file_selector(self, selector: str, available_files: List[str]) -> List[str]:
        """–ü–∞—Ä—Å–∏—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä —Ñ–∞–π–ª–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        selector = selector.strip()
        selected_files = []
        
        # –í—Å–µ —Ñ–∞–π–ª—ã
        if selector == "*":
            return available_files
        
        # –ß–∏—Å–ª–æ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2-5)
        if re.match(r'^\d+-\d+$', selector):
            start, end = map(int, selector.split('-'))
            for i in range(start - 1, min(end, len(available_files))):
                if 0 <= i < len(available_files):
                    selected_files.append(available_files[i])
            return selected_files
        
        # –°–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1,3,5)
        if re.match(r'^[\d,\s]+$', selector):
            numbers = [int(n.strip()) for n in selector.split(',') if n.strip()]
            for num in numbers:
                if 1 <= num <= len(available_files):
                    selected_files.append(available_files[num - 1])
            return selected_files
        
        # –ú–∞—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, *.py –∏–ª–∏ test_*)
        if '*' in selector or '?' in selector:
            pattern = selector.lower()
            for filename in available_files:
                if fnmatch.fnmatch(filename.lower(), pattern):
                    selected_files.append(filename)
            return selected_files
        
        # –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
        filenames = [f.strip() for f in selector.split(',')]
        for filename in filenames:
            if filename in available_files:
                selected_files.append(filename)
            else:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                for available in available_files:
                    if filename.lower() in available.lower():
                        selected_files.append(available)
                        break
        
        return selected_files
    
    def _split_into_chunks(self, content: str, filename: str) -> List[Dict[str, Any]]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º"""
        chunking_config = self.config.get('chunking', {})
        
        if not chunking_config.get('enabled', False):
            return [{'content': content, 'chunk_num': 1, 'total_chunks': 1}]
        
        chunk_size = chunking_config.get('chunk_size', 4000)
        overlap_size = chunking_config.get('overlap_size', 200)
        chunk_mode = chunking_config.get('chunk_mode', 'sliding_window')
        max_chunks = chunking_config.get('max_chunks_per_file', 50)
        
        chunks = []
        
        if chunk_mode == 'fixed':
            # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫—É—Å–∫–∏ –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            for i in range(0, len(content), chunk_size):
                if len(chunks) >= max_chunks:
                    break
                chunks.append(content[i:i + chunk_size])
        
        elif chunk_mode == 'sliding_window':
            # –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
            step = chunk_size - overlap_size
            for i in range(0, len(content), step):
                if len(chunks) >= max_chunks:
                    break
                chunk = content[i:i + chunk_size]
                if len(chunk.strip()) > 0:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏
                    chunks.append(chunk)
        
        elif chunk_mode == 'smart':
            # –£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ª–æ–≥–∏—á–µ—Å–∫–∏–º –≥—Ä–∞–Ω–∏—Ü–∞–º
            smart_boundaries = chunking_config.get('smart_boundaries', {})
            
            if smart_boundaries.get('enabled', True):
                patterns = smart_boundaries.get('patterns', [])
                
                # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–æ–∑–∏—Ü–∏–∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü
                boundaries = [0]
                for pattern in patterns:
                    for match in re.finditer(pattern, content):
                        boundaries.append(match.start())
                
                boundaries.append(len(content))
                boundaries = sorted(set(boundaries))
                
                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–Ω–∏—Ü
                current_chunk = ""
                for i in range(1, len(boundaries)):
                    segment = content[boundaries[i-1]:boundaries[i]]
                    
                    if len(current_chunk) + len(segment) <= chunk_size:
                        current_chunk += segment
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk)
                            if len(chunks) >= max_chunks:
                                break
                        
                        # –ï—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç –±–æ–ª—å—à–µ chunk_size, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
                        if len(segment) > chunk_size:
                            for j in range(0, len(segment), chunk_size):
                                sub_chunk = segment[j:j + chunk_size]
                                if sub_chunk.strip():
                                    chunks.append(sub_chunk)
                                    if len(chunks) >= max_chunks:
                                        break
                            current_chunk = ""
                        else:
                            current_chunk = segment
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
                if current_chunk.strip() and len(chunks) < max_chunks:
                    chunks.append(current_chunk)
            
            else:
                # –ï—Å–ª–∏ —É–º–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –æ—Ç–∫–ª—é—á–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º sliding_window
                return self._split_into_chunks_sliding_window(content, chunk_size, overlap_size, max_chunks)
        
        # –ï—Å–ª–∏ –Ω–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç
        if not chunks:
            chunks = [content]
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        total_chunks = len(chunks)
        result = []
        for i, chunk_content in enumerate(chunks, 1):
            result.append({
                'content': chunk_content,
                'chunk_num': i,
                'total_chunks': total_chunks
            })
        
        return result
    
    def _split_into_chunks_sliding_window(self, content: str, chunk_size: int, overlap_size: int, max_chunks: int) -> List[str]:
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è sliding_window —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è"""
        chunks = []
        step = chunk_size - overlap_size
        
        for i in range(0, len(content), step):
            if len(chunks) >= max_chunks:
                break
            chunk = content[i:i + chunk_size]
            if len(chunk.strip()) > 0:
                chunks.append(chunk)
        
        return chunks
    
    async def _process_concatenate(
        self,
        user_id: str,
        query: str,
        context: Dict[str, Any]
    ) -> Tuple[bool, str, str]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –≤ —Ä–µ–∂–∏–º–µ –ø—Ä–æ—Å—Ç–æ–π —Å–∫–ª–µ–π–∫–∏ –±–µ–∑ –ò–ò"""
        try:
            # –í —Ä–µ–∂–∏–º–µ concatenate –ø—Ä–∏–Ω–∏–º–∞–µ–º –ø—Ä–æ—Å—Ç–æ —Å–µ–ª–µ–∫—Ç–æ—Ä —Ñ–∞–π–ª–æ–≤
            file_selector = query.strip()
            
            if not file_selector:
                return False, "‚ö†Ô∏è –ù–µ —É–∫–∞–∑–∞–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–∫–ª–µ–π–∫–∏", ""
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            codebase_manager = context.get('codebase_manager')
            file_manager = context.get('file_manager')
            
            if not all([codebase_manager, file_manager]):
                return False, "‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞", ""
            
            # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—É—é –∫–æ–¥–æ–≤—É—é –±–∞–∑—É
            user_codebases = await codebase_manager.get_user_codebases(user_id)
            active_codebase_id = user_codebases.get('active')
            
            if not active_codebase_id:
                return False, "‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–π –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã\\. –í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –∫–æ–º–∞–Ω–¥–æ–π /switch", ""
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
            files_data = await file_manager.list_files_for_agent(
                user_id, active_codebase_id, page=1, per_page=1000
            )
            
            if not files_data.get('files'):
                return False, "‚ö†Ô∏è –í –∞–∫—Ç–∏–≤–Ω–æ–π –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤", ""
            
            available_files = [f['name'] for f in files_data['files']]
            
            # –ü–∞—Ä—Å–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä
            selected_files = self._parse_file_selector(file_selector, available_files)
            
            if not selected_files:
                return False, f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {escape_markdown_v2(file_selector)}", ""
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ concatenate
            concat_config = self.config.get('concatenate_settings', {})
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–æ–≤
            sort_mode = concat_config.get('sort_files', 'name')
            if sort_mode == 'name':
                selected_files.sort()
            elif sort_mode == 'extension':
                selected_files.sort(key=lambda f: (f.split('.')[-1] if '.' in f else '', f))
            elif sort_mode == 'size':
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤
                file_sizes = {}
                for filename in selected_files:
                    file_content = await file_manager.get_file_for_agent(
                        user_id, active_codebase_id, filename
                    )
                    if file_content:
                        file_sizes[filename] = len(file_content)
                selected_files.sort(key=lambda f: file_sizes.get(f, 0))
            
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
            if concat_config.get('group_by_extension', False):
                grouped = {}
                for filename in selected_files:
                    ext = filename.split('.')[-1] if '.' in filename else 'no_extension'
                    if ext not in grouped:
                        grouped[ext] = []
                    grouped[ext].append(filename)
                
                # –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
                selected_files = []
                for ext in sorted(grouped.keys()):
                    selected_files.extend(sorted(grouped[ext]))
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
            concatenated_parts = []
            toc_items = []
            total_size = 0
            processed_count = 0
            skipped_count = 0
            
            for idx, filename in enumerate(selected_files, 1):
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
                file_content = await file_manager.get_file_for_agent(
                    user_id, active_codebase_id, filename
                )
                
                if not file_content:
                    skipped_count += 1
                    continue
                
                file_size = len(file_content)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
                max_file_size = concat_config.get('max_file_size', 1048576)
                if file_size > max_file_size and not concat_config.get('truncate_large_files', True):
                    skipped_count += 1
                    continue
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                encoding_config = self.config.get('encoding', {})
                try_encodings = encoding_config.get('try_encodings', ['utf-8', 'cp1251', 'latin-1', 'cp866'])
                fallback_encoding = encoding_config.get('fallback_encoding', 'latin-1')
                fallback_errors = encoding_config.get('fallback_errors', 'ignore')
                
                content = None
                for encoding in try_encodings:
                    try:
                        content = file_content.decode(encoding)
                        break
                    except (UnicodeDecodeError, LookupError):
                        continue
                
                if content is None:
                    try:
                        content = file_content.decode(fallback_encoding, errors=fallback_errors)
                    except:
                        skipped_count += 1
                        continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
                if concat_config.get('skip_empty_files', True) and not content.strip():
                    skipped_count += 1
                    continue
                
                # –û–±—Ä–µ–∑–∞–µ–º –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã
                if concat_config.get('truncate_large_files', True):
                    truncate_at = concat_config.get('truncate_at', 100000)
                    if len(content) > truncate_at:
                        truncate_msg = concat_config.get(
                            'truncate_message',
                            '\n\n[... —Ñ–∞–π–ª –æ–±—Ä–µ–∑–∞–Ω, –ø–æ–∫–∞–∑–∞–Ω–æ {shown} –∏–∑ {total} —Å–∏–º–≤–æ–ª–æ–≤ ...]\n'
                        )
                        truncated_content = content[:truncate_at]
                        truncated_content += truncate_msg.format(
                            shown=truncate_at,
                            total=len(content)
                        )
                        content = truncated_content
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ–∞–π–ª–∞
                separator_line = "=" * 60
                # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —à–∞–±–ª–æ–Ω, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –∫–æ–Ω—Ñ–∏–≥
                header_template = '\n\n' + separator_line + '\n# –§–∞–π–ª: {filename}\n# –†–∞–∑–º–µ—Ä: {size} –±–∞–π—Ç\n' + separator_line + '\n\n'

                # –°–æ–∑–¥–∞–µ–º —è–∫–æ—Ä—å –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
                anchor = filename.replace('/', '_').replace('.', '_').replace(' ', '_')

                header = header_template.format(
                    filename=filename,
                    size=file_size
                )

                # –î–æ–±–∞–≤–ª—è–µ–º —è–∫–æ—Ä—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if concat_config.get('add_file_anchors', True):
                    header = f'<a name="{anchor}"></a>\n{header}'                     
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                file_separator = concat_config.get('file_separator', '\n\n')
                if concatenated_parts:
                    concatenated_parts.append(file_separator)
                concatenated_parts.append(header + content)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
                toc_item_template = concat_config.get(
                    'toc_item_template',
                    '{index}. [{filename}](#{anchor}) - {size} –±–∞–π—Ç\n'
                )
                toc_items.append(toc_item_template.format(
                    index=idx,
                    filename=filename,
                    anchor=anchor,
                    size=file_size
                ))
                
                processed_count += 1
                total_size += file_size
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            result_parts = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
            if concat_config.get('include_toc', True) and toc_items:
                toc_template = concat_config.get(
                    'toc_template',
                    '# –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç\n\n## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{toc_items}\n\n---\n\n'
                )
                toc_content = toc_template.format(toc_items=''.join(toc_items))
                result_parts.append(toc_content)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤
            result_parts.extend(concatenated_parts)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            if concat_config.get('include_stats', True):
                stats_template = concat_config.get(
                    'stats_template',
                    '\n\n---\n\n## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n- –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}\n- –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size}\n- –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {timestamp}\n'
                )
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä
                if total_size < 1024:
                    size_str = f"{total_size} –±–∞–π—Ç"
                elif total_size < 1024 * 1024:
                    size_str = f"{total_size / 1024:.2f} –ö–ë"
                else:
                    size_str = f"{total_size / (1024 * 1024):.2f} –ú–ë"
                
                stats = stats_template.format(
                    total_files=processed_count,
                    total_size=size_str,
                    timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                )
                result_parts.append(stats)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final_result = ''.join(result_parts)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç—É—Å
            status_msg = f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_count}/{len(selected_files)}"
            if skipped_count > 0:
                status_msg += f"\n‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}"
            status_msg += f"\nüìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:,} –±–∞–π—Ç"
            
            return True, status_msg, final_result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ä–µ–∂–∏–º–µ concatenate: {e}", exc_info=True)
            return False, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∫–ª–µ–π–∫–∏ —Ñ–∞–π–ª–æ–≤: {escape_markdown_v2(str(e))}", ""
    
    async def _process_file_with_chunks(
        self,
        user_id: str,
        filename: str,
        content: str,
        ai_query: str,
        ai_interface: Any,
        ai_provider: str,
        ai_model: str,
        ai_params: Dict,
        job_id: str
    ) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —á–∞–Ω–∫–∏"""
        chunking_config = self.config.get('chunking', {})
        chunks = self._split_into_chunks(content, filename)
        
        if len(chunks) == 1:
            # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —á–∞–Ω–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω–æ
            return await self._process_single_content(
                user_id,
                filename, content, ai_query, ai_interface,
                ai_provider, ai_model, ai_params
            )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
        chunk_results = []
        previous_context = ""
        chunk_header_template = chunking_config.get(
            'chunk_header_template',
            "\n[–ß–∞—Å—Ç—å {chunk_num}/{total_chunks} —Ñ–∞–π–ª–∞ {filename}]\n"
        )
        process_sequentially = chunking_config.get('process_chunks_sequentially', False)
        
        for chunk_data in chunks:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–º–µ–Ω—É
            if self.active_jobs[job_id]['cancelled']:
                break
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —á–∞–Ω–∫–∞
            chunk_header = chunk_header_template.format(
                chunk_num=chunk_data['chunk_num'],
                total_chunks=chunk_data['total_chunks'],
                filename=filename
            )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            if process_sequentially and previous_context:
                chunk_context_template = chunking_config.get(
                    'chunk_context_template',
                    "–≠—Ç–æ —á–∞—Å—Ç—å {chunk_num} –∏–∑ {total_chunks} —Ñ–∞–π–ª–∞ {filename}. –ü—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {previous_context}"
                )
                
                context_info = chunk_context_template.format(
                    chunk_num=chunk_data['chunk_num'],
                    total_chunks=chunk_data['total_chunks'],
                    filename=filename,
                    previous_context=previous_context[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                )
                
                system_prompt = f"{context_info}\n\n{chunk_header}\n{chunk_data['content']}"
            else:
                system_prompt = f"–§–∞–π–ª: {filename}\n{chunk_header}\n{chunk_data['content']}"
            
            try:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ò–ò
                response = await ai_interface.send_simple_request(
                    user_id=user_id,
                    provider=ai_provider,
                    prompt=ai_query,
                    system_prompt=system_prompt,
                    model=ai_model if ai_model != 'default' else None,
                    **ai_params
                )
                
                if response:
                    chunk_results.append({
                        'chunk_num': chunk_data['chunk_num'],
                        'result': response,
                        'error': False
                    })
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
                    if process_sequentially:
                        previous_context = response[:500]
                else:
                    chunk_results.append({
                        'chunk_num': chunk_data['chunk_num'],
                        'result': f'–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò –¥–ª—è —á–∞—Å—Ç–∏ {chunk_data["chunk_num"]}',
                        'error': True
                    })
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ {chunk_data['chunk_num']} —Ñ–∞–π–ª–∞ {filename}: {e}")
                chunk_results.append({
                    'chunk_num': chunk_data['chunk_num'],
                    'result': f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–∏ {chunk_data["chunk_num"]}: {str(e)}',
                    'error': True
                })
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(0.3)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–∞–Ω–∫–æ–≤
        if chunking_config.get('combine_chunk_results', True):
            chunk_separator = chunking_config.get('chunk_result_separator', '\n\n--- –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ ---\n\n')
            combined_result = chunk_separator.join([r['result'] for r in chunk_results if not r.get('error')])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏ –≤ –∫–æ–Ω–µ—Ü, –µ—Å–ª–∏ –µ—Å—Ç—å
            errors = [r['result'] for r in chunk_results if r.get('error')]
            if errors:
                combined_result += '\n\n‚ö†Ô∏è –û—à–∏–±–∫–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ:\n' + '\n'.join(errors)
            
            return {
                'filename': filename,
                'result': combined_result,
                'error': any(r.get('error') for r in chunk_results)
            }
        else:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –µ—Å—Ç—å
            return {
                'filename': filename,
                'result': chunk_results,
                'error': any(r.get('error') for r in chunk_results)
            }
    
    async def _process_single_content(
        self,
        user_id: str,
        filename: str,
        content: str,
        ai_query: str,
        ai_interface: Any,
        ai_provider: str,
        ai_model: str,
        ai_params: Dict
    ) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –µ–¥–∏–Ω–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ñ–∞–π–ª –∏–ª–∏ —á–∞–Ω–∫)"""
        system_prompt = f"–§–∞–π–ª: {filename}\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:\n{content}"
        
        try:
            response = await ai_interface.send_simple_request(
                user_id=user_id,
                provider=ai_provider,
                prompt=ai_query,
                system_prompt=system_prompt,
                model=ai_model if ai_model != 'default' else None,
                **ai_params
            )
            
            if response:
                return {
                    'filename': filename,
                    'result': response,
                    'error': False
                }
            else:
                return {
                    'filename': filename,
                    'result': '–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò',
                    'error': True
                }
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
            return {
                'filename': filename,
                'result': f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}',
                'error': True
            }
    
    async def process(
        self,
        user_id: str,
        query: str,
        context: Dict[str, Any]
    ) -> Tuple[bool, str]:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–∏–º concatenate
            concatenate_config = self.config.get('concatenate_settings', {})
            if concatenate_config.get('enabled', False):
                # –í —Ä–µ–∂–∏–º–µ concatenate query —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Å–µ–ª–µ–∫—Ç–æ—Ä —Ñ–∞–π–ª–æ–≤
                return await self._process_concatenate(user_id, query, context)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            if 'virtual_files' in context:
                virtual_files = context['virtual_files']
                lines = query.strip().split('\n', 1)
            
                if len(lines) < 2:
                    return False, "‚å†–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞", ""
            
                file_selector = lines[0].strip()
                ai_query = lines[1].strip()
            
                # –î–ª—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                if file_selector in virtual_files:
                    file_content = virtual_files[file_selector]
                
                    ai_interface = context.get('ai_interface')
                    if not ai_interface:
                        return False, "–ò–ò –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω", ""
                
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
                    ai_settings = self.config.get('ai_settings', {})
                    ai_provider = ai_settings.get('provider', 'default')
                
                    if ai_provider == 'default':
                        if ai_interface.has_api_key('openai'):
                            ai_provider = 'openai'
                        elif ai_interface.has_api_key('anthropic'):
                            ai_provider = 'anthropic'
                        else:
                            return False, "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö API –∫–ª—é—á–µ–π", ""
                
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    ai_params = {}
                    if ai_provider == "openai":
                        ai_params["max_completion_tokens"] = ai_settings.get("max_completion_tokens", 4096)
                    else:
                        ai_params["max_tokens"] = ai_settings.get("max_tokens", 4096)
                    ai_params["temperature"] = ai_settings.get('temperature', 1.0)
                
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                    response = await ai_interface.send_simple_request(
                        user_id=user_id,
                        provider=ai_provider,
                        prompt=ai_query,
                        system_prompt=f"–û–±—Ä–∞–±–æ—Ç–∞–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –∑–∞–ø—Ä–æ—Å—É.\n\n–¢–µ–∫—Å—Ç:\n{file_content}",
                        **ai_params
                    )
                
                    if response:
                        return True, "‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ", response
                    else:
                        return False, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò", ""
            
                return False, f"–í–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª {file_selector} –Ω–µ –Ω–∞–π–¥–µ–Ω", ""

            lines = query.strip().split('\n', 1)
            if len(lines) < 2:
                return False, "‚å†–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞\\. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:\n@filejob —Å–µ–ª–µ–∫—Ç–æ—Ä\\_—Ñ–∞–π–ª–æ–≤\n–∑–∞–ø—Ä–æ—Å –¥–ª—è –ò–ò"
            
            file_selector = lines[0].strip()
            ai_query = lines[1].strip()
            
            if not ai_query:
                return False, "‚å†–ù–µ —É–∫–∞–∑–∞–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤"
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
            codebase_manager = context.get('codebase_manager')
            file_manager = context.get('file_manager')
            ai_interface = context.get('ai_interface')
            
            if not all([codebase_manager, file_manager, ai_interface]):
                return False, "‚å†–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞"
            
            # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—É—é –∫–æ–¥–æ–≤—É—é –±–∞–∑—É
            user_codebases = await codebase_manager.get_user_codebases(user_id)
            active_codebase_id = user_codebases.get('active')
            
            if not active_codebase_id:
                return False, "‚å†–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–π –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã\\. –í—ã–±–µ—Ä–∏—Ç–µ –±–∞–∑—É –∫–æ–º–∞–Ω–¥–æ–π /switch"
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
            files_data = await file_manager.list_files_for_agent(
                user_id, active_codebase_id, page=1, per_page=1000
            )
            
            if not files_data.get('files'):
                return False, "‚å†–í –∞–∫—Ç–∏–≤–Ω–æ–π –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤"
            
            available_files = [f['name'] for f in files_data['files']]
            
            # –ü–∞—Ä—Å–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä –∏ –ø–æ–ª—É—á–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            selected_files = self._parse_file_selector(file_selector, available_files)
            
            if not selected_files:
                return False, f"‚å†–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {escape_markdown_v2(file_selector)}"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
            max_files = self.config.get('max_files_per_job', 50)
            if len(selected_files) > max_files:
                return False, f"‚å†–í—ã–±—Ä–∞–Ω–æ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ñ–∞–π–ª–æ–≤ ({len(selected_files)})\\. –ú–∞–∫—Å–∏–º—É–º: {max_files}"
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∑–∞–¥–∞—á–∏
            job_id = f"{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.active_jobs[job_id] = {
                'user_id': user_id,
                'files': selected_files,
                'query': ai_query,
                'cancelled': False,
                'processed': 0,
                'results': []
            }
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
            results = []
            processing_mode = self.config.get('processing_mode', 'independent')
            chunking_enabled = self.config.get('chunking', {}).get('enabled', False)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ò–ò
            ai_settings = self.config.get('ai_settings', {})
            ai_provider = ai_settings.get('provider', 'default')
            ai_model = ai_settings.get('model', 'default')
            
            if ai_provider == 'default':
                if ai_interface.has_api_key('openai'):
                    ai_provider = 'openai'
                elif ai_interface.has_api_key('anthropic'):
                    ai_provider = 'anthropic'
                else:
                    return False, "‚å†–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö API –∫–ª—é—á–µ–π –¥–ª—è –ò–ò –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ò–ò
            ai_params = {}
            if ai_provider == "openai":
                ai_params["max_completion_tokens"] = ai_settings.get("max_completion_tokens", ai_settings.get("max_tokens", None))
                ai_params["temperature"] = ai_settings.get('temperature', 1.0)
            else:
                ai_params["max_tokens"] = ai_settings.get("max_tokens", ai_settings.get("max_completion_tokens", None))
                ai_params["temperature"] = ai_settings.get('temperature', 1.0)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
            for idx, filename in enumerate(selected_files):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–º–µ–Ω—É
                if self.active_jobs[job_id]['cancelled']:
                    logger.info(f"–ó–∞–¥–∞—á–∞ {job_id} –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                    break
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                self.active_jobs[job_id]['processed'] = idx + 1
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
                file_content = await file_manager.get_file_for_agent(
                    user_id, active_codebase_id, filename
                )
                
                if not file_content:
                    results.append({
                        'filename': filename,
                        'result': '–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª',
                        'error': True
                    })
                    continue
                
                try:
                    content = file_content.decode('utf-8')
                except UnicodeDecodeError:
                    content = file_content.decode('latin-1')
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
                if chunking_enabled:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å —á–∞–Ω–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º
                    result = await self._process_file_with_chunks(
                        user_id,
                        filename, content, ai_query, ai_interface,
                        ai_provider, ai_model, ai_params, job_id
                    )
                else:
                    # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    if processing_mode == 'sequential' and results:
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ñ–∞–π–ª–æ–≤
                        previous_results = "\n\n".join([
                            f"–§–∞–π–ª {r['filename']}:\n{r['result']}" 
                            for r in results if not r.get('error')
                        ])
                        
                        system_prompt = f"""–ü—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏:
{previous_results}

–¢–µ–∫—É—â–∏–π —Ñ–∞–π–ª: {filename}
–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:
{content}"""
                    else:
                        system_prompt = f"–§–∞–π–ª: {filename}\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:\n{content}"
                    
                    try:
                        response = await ai_interface.send_simple_request(
                            user_id=user_id,
                            provider=ai_provider,
                            prompt=ai_query,
                            system_prompt=system_prompt,
                            model=ai_model if ai_model != 'default' else None,
                            **ai_params
                        )
                        
                        if response:
                            result = {
                                'filename': filename,
                                'result': response,
                                'error': False
                            }
                        else:
                            result = {
                                'filename': filename,
                                'result': '–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò',
                                'error': True
                            }
                    
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
                        result = {
                            'filename': filename,
                            'result': f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}',
                            'error': True
                        }
                
                results.append(result)
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(0.5)
            
            # –í–ê–ñ–ù–û: –≠—Ç–æ—Ç –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –í–ù–ï —Ü–∏–∫–ª–∞ for, –Ω–∞ —Ç–æ–º –∂–µ —É—Ä–æ–≤–Ω–µ –æ—Ç—Å—Ç—É–ø–∞!
            # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            temp_dir = None
            archive_path = None

            try:
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                temp_dir = tempfile.mkdtemp(prefix="filejob_")
    
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
                saved_files_count = 0
                for result in results:
                    if not result.get('error'):
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                        base_name, ext = os.path.splitext(result['filename'])
                        if not ext:
                            ext = '.txt'
                        result_filename = f"{base_name}_res.txt"
                        result_path = os.path.join(temp_dir, result_filename)
            
                        try:
                            with open(result_path, 'w', encoding='utf-8') as f:
                                f.write(result['result'])
                            saved_files_count += 1
                            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {result_filename}, —Ä–∞–∑–º–µ—Ä: {len(result['result'])} —Å–∏–º–≤–æ–ª–æ–≤")
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è {result['filename']}: {e}")
                    else:
                        logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª —Å –æ—à–∏–±–∫–æ–π: {result['filename']}")
    
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞: {saved_files_count} –∏–∑ {len(results)}")
    
                # –°–æ–∑–¥–∞–µ–º ZIP –∞—Ä—Ö–∏–≤
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                archive_name = f"filejob_results_{timestamp}.zip"
                archive_path = os.path.join(temp_dir, archive_name)
    
                with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    files_added = 0
                    for root, dirs, files in os.walk(temp_dir):
                        for file in files:
                            if file != archive_name:  # –ù–µ –≤–∫–ª—é—á–∞–µ–º —Å–∞–º –∞—Ä—Ö–∏–≤
                                file_path = os.path.join(root, file)
                                arcname = os.path.relpath(file_path, temp_dir)
                                zipf.write(file_path, arcname)
                                files_added += 1
                    logger.info(f"–§–∞–π–ª–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –∞—Ä—Ö–∏–≤: {files_added}")
    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∞—Ä—Ö–∏–≤–∞
                archive_size = os.path.getsize(archive_path)
                logger.info(f"–°–æ–∑–¥–∞–Ω –∞—Ä—Ö–∏–≤ {archive_name}, —Ä–∞–∑–º–µ—Ä: {archive_size} –±–∞–π—Ç")
    
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞—Ä—Ö–∏–≤ –µ—Å–ª–∏ –µ—Å—Ç—å message –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                message = context.get('message')
                if message and os.path.exists(archive_path):
                    try:
                        file_size = os.path.getsize(archive_path)
                        size_mb = file_size / (1024 * 1024)
            
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        processed = len(results)
                        errors = sum(1 for r in results if r.get('error'))
            
                        status_msg = f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed}/{len(selected_files)}"
                        if errors > 0:
                            status_msg += f"\n‚ö†Ô∏è –° –æ—à–∏–±–∫–∞–º–∏: {errors}"
                        if processed < len(selected_files):
                            status_msg += f"\n‚ö†Ô∏è –ó–∞–¥–∞—á–∞ –±—ã–ª–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞"
                        status_msg += f"\nüì¶ –†–∞–∑–º–µ—Ä –∞—Ä—Ö–∏–≤–∞: {size_mb:.2f} –ú–ë"
            
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞—Ä—Ö–∏–≤
                        document = FSInputFile(archive_path, filename=archive_name)
                        await message.reply_document(
                            document,
                            caption=f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ {processed} —Ñ–∞–π–ª–æ–≤\n–ó–∞–ø—Ä–æ—Å: {ai_query[:100]}..."
                        )
            
                        logger.info(f"–ê—Ä—Ö–∏–≤ {archive_name} —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω")
            
                        # –£–¥–∞–ª—è–µ–º –∑–∞–¥–∞—á—É –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
                        if job_id in self.active_jobs:
                            del self.active_jobs[job_id]
            
                        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å–ø–µ—Ö –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–∞—Ä—Ö–∏–≤ —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω)
                        return True, status_msg, None
            
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—Ä—Ö–∏–≤–∞: {e}")
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ–±—ã—á–Ω—ã–º –≤–æ–∑–≤—Ä–∞—Ç–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞—Ä—Ö–∏–≤–∞: {e}")
    
            finally:
                # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                if temp_dir and os.path.exists(temp_dir):
                    try:
                        await asyncio.sleep(2)
                        shutil.rmtree(temp_dir)
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∞—Ä—Ö–∏–≤ –Ω–µ –±—ã–ª –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω)
            final_result = self._format_results(results)

            # –£–¥–∞–ª—è–µ–º –∑–∞–¥–∞—á—É –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            if job_id in self.active_jobs:
                del self.active_jobs[job_id]

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            processed = len(results)
            errors = sum(1 for r in results if r.get('error'))

            status_msg = f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed}/{len(selected_files)}"
            if errors > 0:
                status_msg += f"\n‚ö†Ô∏è –° –æ—à–∏–±–∫–∞–º–∏: {errors}"
            if processed < len(selected_files):
                status_msg += f"\n‚ö†Ô∏è –ó–∞–¥–∞—á–∞ –±—ã–ª–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞"

            return True, status_msg, final_result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ Filejob –∞–≥–µ–Ω—Ç–µ: {e}", exc_info=True)
            return False, f"‚å†–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {escape_markdown_v2(str(e))}"
    
    def _format_results(self, results: List[Dict]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∏—Ç–æ–≥–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
        separator = self.config.get('result_separator', '\n\n---\n\n')
        header_template = self.config.get('file_header_template', '# –§–∞–π–ª: {filename}\n\n')
        include_source = self.config.get('include_source_in_result', False)
        
        formatted_parts = []
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è merged —Ä–µ–∂–∏–º–∞
        if len(results) == 1 and results[0].get('filename') == 'merged_result':
            return results[0]['result']
        
        # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è independent –∏ sequential —Ä–µ–∂–∏–º–æ–≤
        for result in results:
            header = header_template.format(
                filename=result['filename'],
                timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            )
            
            content = header + result['result']
            formatted_parts.append(content)
        
        return separator.join(formatted_parts)
    
    def cancel_job(self, job_id: str) -> bool:
        """–û—Ç–º–µ–Ω—è–µ—Ç –∞–∫—Ç–∏–≤–Ω—É—é –∑–∞–¥–∞—á—É"""
        if job_id in self.active_jobs:
            self.active_jobs[job_id]['cancelled'] = True
            return True
        return False
    
    def get_job_status(self, job_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏"""
        return self.active_jobs.get(job_id)
    
    def get_config(self) -> Dict[str, Any]:
        return self.config.copy()
    
    def set_config(self, config: Dict[str, Any]):
        self.config = config